{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcbcdJ8imI1T",
        "outputId": "9e21e34d-c2c4-4405-9e67-1daa427064ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ov451U6FoSdV",
        "outputId": "1c48cb37-4318-4b19-a315-b116c81b8f01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dask_ml\n",
            "  Downloading dask_ml-2023.3.24-py3-none-any.whl (148 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m148.7/148.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dask[array,dataframe]>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from dask_ml) (2022.12.1)\n",
            "Requirement already satisfied: distributed>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from dask_ml) (2022.12.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from dask_ml) (0.56.4)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from dask_ml) (1.22.4)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from dask_ml) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask_ml) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from dask_ml) (1.10.1)\n",
            "Collecting dask-glm>=0.2.0 (from dask_ml)\n",
            "  Downloading dask_glm-0.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: multipledispatch>=0.4.9 in /usr/local/lib/python3.10/dist-packages (from dask_ml) (0.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from dask_ml) (23.1)\n",
            "Requirement already satisfied: cloudpickle>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from dask-glm>=0.2.0->dask_ml) (2.2.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (8.1.3)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (2023.4.0)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.10/dist-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (1.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (6.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (0.12.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask_ml) (3.1.2)\n",
            "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask_ml) (1.0.0)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask_ml) (1.0.5)\n",
            "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask_ml) (5.9.5)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask_ml) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask_ml) (1.7.0)\n",
            "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask_ml) (6.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask_ml) (1.26.15)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask_ml) (3.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from multipledispatch>=0.4.9->dask_ml) (1.16.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->dask_ml) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->dask_ml) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->dask_ml) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->dask_ml) (2022.7.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2.0->dask_ml) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2.0->dask_ml) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->distributed>=2.4.0->dask_ml) (2.1.2)\n",
            "Installing collected packages: dask-glm, dask_ml\n",
            "Successfully installed dask-glm-0.2.0 dask_ml-2023.3.24\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993243 sha256=36b5bd40a2d53693bccfd8e873a41bae5e3fbb713321660be11f4c20f82df065\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m240.9/240.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=ab7cd6470b2e5ed36f8a5948f764cfd36a8ae8688a0413675c35469d3d9a5573\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/3d/88/51a592b9ad17e7899126563698b4e3961983ebe85747228ba6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.2.0\n",
            "                                                Text         Username  \\\n",
            "1  glad maliaðŸ‘â£sadly seems media main culprits co...     Kate18660449   \n",
            "2  lot shit happen await sky pirates info lt3 tan...     vanetheworld   \n",
            "4  yep pete dropping raises likelihood viability ...      MaxKennerly   \n",
            "5  using land farming timber natural resources ma...   killinger_lexi   \n",
            "6  ðŸ“seminar mexico qampa thread olympics people s...  MedvedevaNation   \n",
            "\n",
            "   depression_stress_score  vader_sentiment  emojie_stress_score  \n",
            "1                        2          -0.4939                    0  \n",
            "2                        0          -0.4939                    0  \n",
            "4                        0           0.5994                    0  \n",
            "5                        3           0.4215                    0  \n",
            "6                        2          -0.6908                    0  \n"
          ]
        }
      ],
      "source": [
        "!pip3 install dask_ml\n",
        "!pip3 install langdetect\n",
        "import dask.dataframe as dd\n",
        "from dask_ml.feature_extraction.text import CountVectorizer\n",
        "from dask_ml.preprocessing import StandardScaler\n",
        "from dask_ml.model_selection import train_test_split\n",
        "from dask_ml.model_selection import GridSearchCV\n",
        "from dask_ml.linear_model import LogisticRegression\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import string\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from langdetect import detect\n",
        "!pip3 install emoji\n",
        "import emoji\n",
        "\n",
        "depression_stress_words = {\n",
        "    'anxious', 'lonely', 'hopeless', 'empty', 'helpless', 'worthless', 'tired', 'exhausted', 'fatigued', 'miserable',\n",
        "    'hopelessness', 'numb', 'irritable', 'sorrow', 'suffering', 'suicidal', 'tearful', 'troubled', 'vulnerable', 'crisis',\n",
        "    'despair', 'disheartened', 'dismal', 'downhearted', 'forlorn', 'gloomy', 'grief', 'melancholy', 'mournful', 'sad',\n",
        "    'tragic', 'unhappy', 'dejected', 'demoralized', 'devastated', 'discouraged', 'downtrodden', 'low-spirited', 'blue',\n",
        "    'heartbroken', 'hurt', 'injured', 'pained', 'distressed', 'tormented', 'angry', 'annoyed', 'frustrated', 'irritated',\n",
        "    'fearful', 'afraid', 'scared', 'terrified', 'worried', 'concerned', 'anxiety', 'panic', 'stress', 'depression', 'suicide',\n",
        "    'self-harm', 'overwhelmed', 'pressure', 'mental health', 'loneliness', 'isolation', 'disconnected', 'unloved', 'unworthy',\n",
        "    'worthlessness', 'worthless', 'rejected', 'failure', 'inadequate', 'guilty', 'ashamed', 'self-blame', 'negativity', 'hopeless',\n",
        "    'meaningless', 'numbness', 'apathy', 'loss', 'trapped', 'confused', 'disoriented', 'disillusioned', 'betrayed', 'humiliated',\n",
        "    'empty', 'exhausted', 'fatigue', 'irritability', 'anger', 'rage', 'mood swings', 'insomnia', 'sleep problems', 'nightmares',\n",
        "    'loss of interest', 'withdrawal', 'social withdrawal', 'lethargic', 'energy loss', 'pessimism', 'dysphoria', 'bad mood',\n",
        "    'foggy', 'lethargy', 'inability to concentrate'\n",
        "}\n",
        "\n",
        "\n",
        "stress_scores = {'death': 5, 'emergency': 4, 'urgent': 4, \n",
        "                           'crisis': 4, 'trauma': 4, 'disaster': 4, \n",
        "                           'injury': 4, 'hospital': 4, 'catastrophe': 4, \n",
        "                           'danger': 3, 'threat': 3, 'violence': 3, 'crime': 3,\n",
        "                           'attack': 3, 'fear': 3, 'panic': 3, 'anxiety': 3, \n",
        "                           'terror': 3, 'conflict': 2, 'pressure': 1, \n",
        "                           'deadline': 2, 'overwhelmed': 2, 'frustrated': 2, \n",
        "                           'exhausted': 2, 'tired': 2, 'fatigue': 2, 'stress': 2, \n",
        "                           'anxious': 2, 'challenging': 1, 'difficult': 1, 'complicated': 1,\n",
        "                           'busy': 1, 'demanding': 1, 'responsibility': 1, \n",
        "                           'workload': 1, 'expectations': 1, 'change': 1,\n",
        "                           'uncertainty': 1, 'unknown': 1, 'new': 1, \n",
        "                           'fired': 3, 'laid off': 3, 'termination': 3,\n",
        "                           'budget cuts': 2, 'bankruptcy': 4, 'divorce': 4, \n",
        "                           'break-up': 3, 'death of a loved one': 4, \n",
        "                           'health issues': 3, 'major life changes': 2, \n",
        "                           'financial problems': 3, 'relationship issues': 2, \n",
        "                           'loneliness': 2, 'isolation': 2, 'rejection': 2, \n",
        "                           'failure': 2, 'disappointment': 1, 'homesick': 1, \n",
        "                           'missing someone': 1, 'nostalgia': 1, 'gratitude': -2, \n",
        "                           'joy': -2, 'happiness': -2, 'relief': -2, 'satisfaction': -2, \n",
        "                           'contentment': -2, 'calm': -2, 'peace': -2, 'tranquility': -2, \n",
        "                           'meditation': -2, 'mindfulness': -2, 'positivity': -2, 'optimism': -2,\n",
        "                           'hope': -2, 'grateful': -1, 'happy': -1, 'excited': -1, 'adventurous': -1, \n",
        "                           'curious': -1, 'motivated': -1, 'enthusiastic': -1, 'confident': -1, \n",
        "                           'proud': -1, 'inspired': -1, 'anger': 3, 'rage': 4, 'irritation': 2,\n",
        "                           'annoyance': 2, 'disgust': 3, 'envy': 2, 'jealousy': 2, 'guilt': 2, \n",
        "                           'stressed': 2, 'lonely': 1, 'hopeless': 5, 'empty': 1, 'helpless': 4, \n",
        "                           'worthless': 4, 'fatigued': 2, 'miserable': 3, 'hopelessness': 4, \n",
        "                           'numb': 3, 'irritable': 2, 'sorrow': 3, 'suffering': 4, \n",
        "                           'suicidal': 5, 'tearful': 3, 'troubled': 2, 'vulnerable': 2, \n",
        "                           'despair': 4, 'disheartened': 3, 'dismal': 3, 'downhearted': 3, \n",
        "                           'forlorn': 3, 'gloomy': 3, 'grief': 4, 'melancholy': 4, 'mournful': 4, \n",
        "                           'sad': 3, 'tragic': 5, 'unhappy': 2, 'dejected': 3, 'demoralized': 4, \n",
        "                           'devastated': 5, 'discouraged': 3, 'downtrodden': 4, 'low-spirited': 3, \n",
        "                           'blue': 3, 'heartbroken': 4, 'hurt': 2, 'injured': 2, 'pained': 2, \n",
        "                           'distressed': 3, 'tormented': 4, 'angry': 2, 'annoyed': 1, 'irritated': 2,\n",
        "                           'fearful': 3, 'afraid': 3, 'scared': 4, 'terrified': 5, 'worried': 2, \n",
        "                           'concerned': 2, 'depression': 5, 'suicide': 5, 'self-harm': 4, \n",
        "                           'mental health': 4, 'disconnected': 2, 'unloved': 3, 'unworthy': 3, \n",
        "                           'worthlessness': 4, 'rejected': 3, 'inadequate': 3, 'guilty': 3, \n",
        "                           'ashamed': 3, 'self-blame': 4, 'negativity': 3, 'meaningless': 3, \n",
        "                           'numbness': 3, 'apathy': 1, 'loss': 3, 'trapped': 4, 'confused': 2, \n",
        "                           'disoriented': 2, 'disillusioned': 2, 'betrayed': 3, 'humiliated': 3, \n",
        "                           'irritability': 2, 'mood swings': 2, 'insomnia': 3, 'sleep problems': 2, \n",
        "                           'nightmares': 3, 'loss of interest': 2, 'withdrawal': 2, 'social withdrawal': 2,\n",
        "                           'lethargic': 2, 'energy loss': 2, 'pessimism': 2, 'dysphoria': 3, 'bad mood': 1,\n",
        "                           'foggy': 2, 'lethargy': 2, 'inability to concentrate': 2}\n",
        "\n",
        "\n",
        "stress_emojis={\n",
        "    \"\\U0001F600\": -2, # grinning face\n",
        "    \"\\U0001F603\": -2, # grinning face with big eyes\n",
        "    \"\\U0001F604\": -2, # grinning face with smiling eyes\n",
        "    \"\\U0001F601\": -2, # beaming face with smiling eyes\n",
        "    \"\\U0001F606\": -2, # grinning squinting face\n",
        "    \"\\U0001F605\": 1, # grinning face with sweat\n",
        "    \"\\U0001F923\": -1, # rolling on the floor crying\n",
        "    \"\\U0001F602\": -1, # face with tears of joy\n",
        "    \"\\U0001F642\": -2, # slightly smiling face\n",
        "    \"\\U0001F643\": 3, # upside-down face\n",
        "    \"\\U0001FAE0\": 1, # meltng face\n",
        "    \"\\U0001F609\": -2, # winking face\n",
        "    \"\\U0001F60A\": -2, # smiling face with smiling eyes\n",
        "    \"\\U0001F607\": -2, # smiling face with halo\n",
        "    \"\\U0001F970\": -2, # smiling face with hearts\n",
        "    \"\\U0001F60D\": -2, # smiling face with heart eyes\n",
        "    \"\\U0001F929\": -2, # star-struck\n",
        "    \"\\U0001F618\": -2, # face blowing a kiss\n",
        "    \"\\U0001F617\": -2, # kissing face\n",
        "    \"\\U0001F61A\": -2, # kissing face with eyes closed\n",
        "    \"\\U0001F619\": -2, # kissing face with smiling eyes\n",
        "    \"\\U0001F972\": 2, # smiling face with tear\n",
        "    \"\\U0001F60B\": -2, # face savoring food\n",
        "    \"\\U0001F61B\": -2, # face with tongue\n",
        "    \"\\U0001F61C\": -1, # winking face with tongue\n",
        "    \"\\U0001F929\": -1, # zany face\n",
        "    \"\\U0001F61D\": -1, # squinting face with tongue\n",
        "    \"\\U0001F911\": -2, # money-mouth face\n",
        "    \"\\U0001F917\": -1, # smiling face with hands\n",
        "    \"\\U0001F92D\": -1, # face with hand over mouth\n",
        "    \"\\U0001FAE2\": 2, # face with opne eyes and hand over mouth\n",
        "    \"\\U0001FAE3\": 3, # face with peeking eye\n",
        "    \"\\U0001F92B\": 1, # shushing face\n",
        "    \"\\U0001F914\": 2, # thinking face\n",
        "    \"\\U0001FAE1\": -1, # saltuing face\n",
        "    \"\\U0001F910\": 1, # zipper-mouth face\n",
        "    \"\\U0001F928\": 2, #face with raising eyebrow\n",
        "    \"\\U0001F610\": 2, # neutral face\n",
        "    \"\\U0001F611\": 1, # expresionless face\n",
        "    \"\\U0001F636\": 1, # face without mouth\n",
        "    \"\\U0001FAE5\": 1, # dotted line face\n",
        "    \"\\U0001F636\": -1, # face in the clouds\n",
        "    \"\\U0001F60F\": -2, # smirk\n",
        "    \"\\U0001F612\": 2, # unamused face\n",
        "    \"\\U0001F644\": 3, # face with rolling eyes\n",
        "    \"\\U0001F62C\": 2, # grimacing face\n",
        "    \"\\U0001F62E\": 2, # face exhaling\n",
        "    \"\\U0001F925\": 1, # lying face\n",
        "    \"\\U0001F60C\": -2, # relieved face\n",
        "    \"\\U0001F614\": 5, # pensive face\n",
        "    \"\\U0001F62A\": 4, # sleepy face\n",
        "    \"\\U0001F924\": -2, # drooling face\n",
        "    \"\\U0001F634\": 1, # sleeping face\n",
        "    \"\\U0001F637\": -2, # face with medical mask \n",
        "    \"\\U0001F912\": -2, # face with thermometer\n",
        "    \"\\U0001F915\": -1, # face with bandage\n",
        "    \"\\U0001F922\": -1, # nauseated face\n",
        "    \"\\U0001F92E\": -1, # face vomiting\n",
        "    \"\\U0001F927\": -2, # sneezing face\n",
        "    \"\\U0001F975\": -2, # hot face\n",
        "    \"\\U0001F976\": -2, # cold face\n",
        "    \"\\U0001F974\": 1, # woozy face\n",
        "    \"\\U0001F635\": -1, # face with crossed-out-eyes\n",
        "    \"\\U0001F92F\": 2, # exploding head\n",
        "    \"\\U0001F60E\": -2, # smiling face with sunglasses\n",
        "    \"\\U0001F913\": -2, # nerd face\n",
        "    \"\\U0001F615\": 1, # confused face\n",
        "    \"\\U0001FAE4\": 2, # face with diagonal mouth\n",
        "    \"\\U0001F61F\": 3, # worried face\n",
        "    \"\\U0001F641\": 3, # slightly frowning face\n",
        "    \"\\U0001F62E\": 1, # face with open mouth\n",
        "    \"\\U0001F62F\": 1, # hushed face \n",
        "    \"\\U0001F633\": 2, # flushed face\n",
        "    \"\\U0001F97A\": -1, # pleading face\n",
        "    \"\\U0001F979\": 2, # face holding back tears\n",
        "    \"\\U0001F626\": 2, # frowning face with open mouth\n",
        "    \"\\U0001F627\": 2, # anguished face\n",
        "    \"\\U0001F62B\": 2, # fearful face\n",
        "    \"\\U0001F630\": 5, # anxious face with sweat\n",
        "    \"\\U0001F625\": 5, # sad but relieved face\n",
        "    \"\\U0001F622\": 4, # crying face\n",
        "    \"\\U0001F62D\": 4, # loudly crying face\n",
        "    \"\\U0001F631\": 3, # face screaming in fear\n",
        "    \"\\U0001F616\": 3, # confused face\n",
        "    \"\\U0001F623\": 5, # perservering face\n",
        "    \"\\U0001F61E\": 5, # dissapointed face\n",
        "    \"\\U0001F613\": 5, # downcast face with sweat\n",
        "    \"\\U0001F629\": 3, # weary face\n",
        "    \"\\U0001F62B\": 3, # tired face\n",
        "    \"\\U0001F971\": 2, # yawnging face\n",
        "    \"\\U0001F624\": 2, # face with steam from nose\n",
        "    \"\\U0001F621\": 1, # enraged face\n",
        "    \"\\U0001F620\": 1, # angry face\n",
        "    \"\\U0001F92C\": 3, # face with symbols on mouth\n",
        "    \"\\U0001F63F\": 4, # crying cat\n",
        "    \"\\U0001F63E\": 4, # pouting cat\n",
        "    \"\\U0001F494\": 4, # broken heart\n",
        "    \"\\U0001F44D\": -2, # thumbs up\n",
        "    \"\\U0001F44E\": 3, # thumbs down\n",
        "\n",
        "}\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "stop_pattern = re.compile(r'\\b(' + '|'.join(stop_words) + r')\\b\\s*')\n",
        "mention_pattern = re.compile(r'@\\w+')\n",
        "hashtag_pattern = re.compile(r'#\\w+')\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                                   \"]+\", flags=re.UNICODE)\n",
        "def calculate_score(text):\n",
        "    score = 0\n",
        "    words = text.lower().split()\n",
        "    for word in words:\n",
        "        if word in stress_scores:\n",
        "            score += stress_scores[word]\n",
        "    # Keep score between -5 and 5\n",
        "    if score > 5:\n",
        "        score = 5\n",
        "    elif score < -5:\n",
        "        score = -5\n",
        "    return score\n",
        "\n",
        "\n",
        "# emoji score \n",
        "def emoji_score(text):\n",
        "    score = 0\n",
        "    words = text.lower().split()\n",
        "    for char in words:\n",
        "        if char in stress_emojis:\n",
        "            score += stress_emojis[char]\n",
        "    if score > 5:\n",
        "      score = 5\n",
        "    elif score < -2:\n",
        "        score = -2\n",
        "    return score\n",
        "\n",
        "\n",
        "def custom_tokenize(text):\n",
        "    try:\n",
        "      lang = detect(text)\n",
        "    except:\n",
        "      lang = \"error\"\n",
        "      print(\"This tweet throws an error:\", text)\n",
        "    \n",
        "    if lang == \"en\":\n",
        "    # Remove URLs\n",
        "        text = url_pattern.sub('', str(text))\n",
        "        # Remove mentions\n",
        "        text = mention_pattern.sub('', str(text))\n",
        "        # Remove hashtags, but keep the words\n",
        "        text = hashtag_pattern.sub('', str(text))\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "        # Remove punctuation\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        # Tokenize tweet into words\n",
        "        words = text.split()\n",
        "        # Remove stop words\n",
        "        words = [word for word in words if word not in stop_words]\n",
        "        return \" \".join(words)\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "# chunk_size = 50000\n",
        "# clean_tweets = []\n",
        "#outputf = \"stress_depression_aylene1.csv\"\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Read the CSV file using Dask\n",
        "df = dd.read_csv('/content/drive/MyDrive/Colab Notebooks/stress_depression9_aylene.csv', usecols=[\"Text\", \"Username\"], low_memory=False, lineterminator='\\n')\n",
        "\n",
        "# Drop rows with at least one missing value\n",
        "df = df.map_partitions(lambda partition: partition.dropna(how='any'))\n",
        "# df = df.map_partitions(lambda df: df.dropna(subset=['Text',\"Username\"]), meta=df)\n",
        "# Filter out rows that don't contain any depression words\n",
        "df = df[df['Text'].str.contains('|'.join(depression_stress_words))]\n",
        "# Tokenize tweets into words\n",
        "df['Text'] = df['Text'].apply(custom_tokenize, meta=('Text', 'object'))\n",
        "# Add a new column to the dataframe with the depression/stress score\n",
        "df['depression_stress_score'] = df['Text'].apply(calculate_score, meta=('x', 'f8'))\n",
        "\n",
        "# Calculate VADER sentiment for each partition of the dataframe\n",
        "df['vader_sentiment'] = df.map_partitions(lambda part: part['Text'].apply(lambda x: analyzer.polarity_scores(x)['compound']))\n",
        "df['emojie_stress_score'] = df['Text'].apply(emoji_score, meta=('x', 'f8'))\n",
        "# Write cleaned data to CSV file\n",
        "df.to_csv(\"stress_depression9_aylene9.csv\", single_file=True, index=False)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "syekBe0mMSrH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv('stress_depression9_aylene9.csv')\n",
        "\n",
        "# drop rows where the 'text' column is empty\n",
        "df.dropna(subset=['Text'], inplace=True)\n",
        "\n",
        "# write the updated DataFrame back to a new CSV file\n",
        "df.to_csv('stress_depression9_aylene9-1.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}